{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate mock community classification accuracy\n",
    "The purpose of this notebook is to evaluate taxonomic classification accuracy of mock communities using different classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment\n",
    "-----------------------\n",
    "\n",
    "First we'll import various functions that we'll need for generating the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os.path import join, exists, expandvars\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import seaborn.colors.xkcd_rgb as colors\n",
    "from tax_credit.plotting_functions import (pointplot_from_data_frame,\n",
    "                                           boxplot_from_data_frame,\n",
    "                                           heatmap_from_data_frame,\n",
    "                                           per_level_kruskal_wallis,\n",
    "                                           beta_diversity_pcoa,\n",
    "                                           average_distance_boxplots,\n",
    "                                           rank_optimized_method_performance_by_dataset)\n",
    "from tax_credit.eval_framework import (evaluate_results,\n",
    "                                       method_by_dataset_a1,\n",
    "                                       parameter_comparisons,\n",
    "                                       merge_expected_and_observed_tables,\n",
    "                                       filter_df,\n",
    "                                       evaluate_unifrac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure local environment-specific values\n",
    "-------------------------------------------\n",
    "\n",
    "**This is the only cell that you will need to edit to generate basic reports locally.** After editing this cell, you can run all cells in this notebook to generate your analysis report. This will take a few minutes to run, as results are computed at multiple taxonomic levels.\n",
    "\n",
    "Values in this cell will not need to be changed, with the exception of ``project_dir``, to generate the default results contained within tax-credit. To analyze results separately from the tax-credit precomputed results, other variables in this cell will need to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## project_dir should be the directory where you've downloaded (or cloned) the \n",
    "## tax-credit repository. \n",
    "project_dir = expandvars(\"../../\")\n",
    "\n",
    "## expected_results_dir contains expected composition data in the structure\n",
    "## expected_results_dir/<dataset name>/<reference name>/expected/\n",
    "expected_results_dir = join(project_dir, \"data/precomputed-results/\", \"mock-community\")\n",
    "\n",
    "## mock_results_fp designates the files to which summary results are written.\n",
    "## If this file exists, it can be read in to generate results plots, instead\n",
    "## of computing new scores.\n",
    "mock_results_fp = join(expected_results_dir, 'mock_results.tsv')\n",
    "\n",
    "## results_dirs should contain the directory or directories where\n",
    "## results can be found. By default, this is the same location as expected \n",
    "## results included with the project. If other results should be included, \n",
    "## absolute paths to those directories should be added to this list.\n",
    "results_dirs = [expected_results_dir]\n",
    "\n",
    "## directory containing mock community data, e.g., feature table without taxonomy\n",
    "mock_dir = join(project_dir, \"data\", \"mock-community\")\n",
    "\n",
    "## Minimum number of times an OTU must be observed for it to be included in analyses. Edit this\n",
    "## to analyze the effect of the minimum count on taxonomic results.\n",
    "min_count = 1\n",
    "\n",
    "## Define the range of taxonomic levels over which to compute accuracy scores.\n",
    "## The default given below will compute order (level 2) through species (level 6)\n",
    "taxonomy_level_range = range(2,7)\n",
    "\n",
    "\n",
    "# we can save plots in this directory\n",
    "outdir = join(expandvars(\"../../\"), 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = ['mock-1', 'mock-2', 'mock-12', 'mock-13', 'mock-14', 'mock-15', 'mock-16', 'mock-18', 'mock-19', \n",
    "               'mock-20', 'mock-21', 'mock-22', 'mock-23']\n",
    "method_ids = ['mindivlp', 'mindivlp_thresh', 'rdp', 'sortmerna', 'uclust', 'blast', 'blast+', 'naive-bayes', 'naive-bayes-bespoke', 'vsearch', 'mindivlp', 'mindivlp_thresh']\n",
    "ref_ids = ['gg_13_8_otus']#, 'unite_20.11.2016_clean_fullITS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find mock community pre-computed tables, expected tables, and \"query\" tables\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables).\n",
    "\n",
    "**Note**: if you have added additional methods to add, set `append=True`. If you are attempting to recompute pre-computed results, set `force=True`.\n",
    "\n",
    "This cell will take a few minutes to run if new results are being added, so hold onto your hat. If you are attempting to re-compute everything, it may take an hour or so, so go take a nap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_results = evaluate_results(results_dirs, \n",
    "                                expected_results_dir, \n",
    "                                mock_results_fp, \n",
    "                                mock_dir,\n",
    "                                taxonomy_level_range=range(2,7), \n",
    "                                min_count=min_count,\n",
    "                                taxa_to_keep=None, \n",
    "                                md_key='taxonomy', \n",
    "                                subsample=False,\n",
    "                                per_seq_precision=True,\n",
    "                                exclude=['other'],\n",
    "                                dataset_ids=dataset_ids,\n",
    "                                reference_ids=ref_ids,\n",
    "                                method_ids=method_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict analyses to a set of datasets or references: e.g., exclude taxonomy assignments made for purpose of reference database comparisons. This can be performed as shown below — alternatively, specific reference databases, datasets, methods, or parameters can be chosen by setting dataset_ids, reference_ids, method_ids, and parameter_ids in the evaluate_results command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mock_results = filter_df(mock_results, column_name='Parameters', values=['10000_14_0.01_8'], exclude=False)\n",
    "mock_results = mock_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Unifrac Distances and Plot\n",
    "--------------------------\n",
    "Compute unifrac distances using evaluate_unifrac, which queries the 99 OTU taxonomy file for OTU IDs and computes unifrac using EMDUnifrac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taxonomy_fp = join(project_dir, \"data/ref_dbs/gg_13_8_otus/99_otu_taxonomy.txt\")\n",
    "tree_fp = join(project_dir, \"data/ref_dbs/gg_13_8_otus/trees/99_otus_unannotated.tree\")  # May vary\n",
    "\n",
    "df = evaluate_unifrac(\n",
    "    taxonomy_fp,\n",
    "    tree_fp,\n",
    "    results_dirs, \n",
    "    expected_results_dir, \n",
    "    mock_results_fp, \n",
    "    mock_dir,\n",
    "    min_count=min_count,\n",
    "    taxa_to_keep=None, \n",
    "    md_key='taxonomy', \n",
    "    subsample=False,\n",
    "    dataset_ids=dataset_ids,\n",
    "    reference_ids=ref_ids,\n",
    "    method_ids=method_ids)\n",
    "\n",
    "df.to_csv(join(project_dir, \"data/precomputed-results/mock-community/unifrac_data.tsv\"), sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load in unifrac distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(project_dir, \"data/precomputed-results/mock-community/unifrac_data.tsv\"), sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unwanted parameters from MinDivLP results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicestoremove = list(df[ (df[\"Method\"] == \"mindivlp\") & (df[\"Parameters\"] != \"10000_14_0.01_8\") ].index.values)\n",
    "indicestoremove += list(df[ (df[\"Method\"] == \"mindivlp_thresh\") & (df[\"Parameters\"] != \"10000_14_0.01_8\") ].index.values)\n",
    "indicestoremove.sort()\n",
    "new_results = df.drop(indicestoremove)\n",
    "new_results[new_results[\"Method\"] == \"mindivlp\"][\"Parameters\"].unique()\n",
    "old_results = df\n",
    "df = new_results\n",
    "df[\"Parameters\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette={\n",
    "    'expected': 'black', 'rdp': colors['baby shit green'], 'sortmerna': colors['macaroni and cheese'],\n",
    "    'uclust': 'coral', 'blast': 'indigo', 'blast+': colors['electric purple'], 'naive-bayes': 'dodgerblue',\n",
    "    'naive-bayes-bespoke': 'blue', 'vsearch': 'firebrick', 'mindivlp': colors['electric green'], 'mindivlp_thresh': colors['black']\n",
    "}\n",
    "\n",
    "y_vars = [\"Unweighted Unifrac\", \"Weighted Unifrac\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = pointplot_from_data_frame(df, \"Dataset\", [\"weighted-unifrac\", \"unweighted-unifrac\"], \n",
    "                                  group_by=\"Reference\", color_by=\"Method\",\n",
    "                                  color_palette=color_palette)\n",
    "# Resize for better viewing\n",
    "point['weighted-unifrac'].fig.set_figheight(6)\n",
    "point['weighted-unifrac'].fig.set_figwidth(14)\n",
    "point['unweighted-unifrac'].fig.set_figheight(6)\n",
    "point['unweighted-unifrac'].fig.set_figwidth(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point['unweighted-unifrac'].fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point['weighted-unifrac'].fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in point.items():\n",
    "    v.savefig(join(outdir, '{0}-lineplots.png'.format(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and summarize precision, recall, and F-measure for mock communities\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the mock communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures. **Higher scores = better accuracy**\n",
    "\n",
    "As a first step, we will evaluate **average** method performance at each taxonomic level for each method within each reference dataset type.\n",
    "\n",
    "**Note that, as parameter configurations can cause results to vary widely, average results are not a good representation of the \"best\" results. See [here](#Optimized-method-performance) for results using optimized parameters for each method.**\n",
    "\n",
    "First we will define our [color palette](http://matplotlib.org/examples/color/named_colors.html) and the variables we want to plot. Via seaborn, we can apply the [xkcd crowdsourced color names](https://xkcd.com/color/rgb.txt). If that still doesn't match your hue, use hex codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette={\n",
    "    'expected': 'black', 'rdp': colors['baby shit green'], 'sortmerna': colors['macaroni and cheese'],\n",
    "    'uclust': 'coral', 'blast': 'indigo', 'blast+': colors['electric purple'], 'naive-bayes': 'dodgerblue',\n",
    "    'naive-bayes-bespoke': 'blue', 'vsearch': 'firebrick', 'mindivlp': colors['electric green'], 'mindivlp_thresh': colors['burnt orange']\n",
    "}\n",
    "\n",
    "y_vars = [\"Taxon Accuracy Rate\", \"Taxon Detection Rate\", \"L1 Error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional for removing unwanted methods.\n",
    "\n",
    "indicestoremove = list(mock_results[ (mock_results[\"Method\"] == \"mindivlp\") & (mock_results[\"Parameters\"] != \"10000_14_0.01_8\") ].index.values)\n",
    "indicestoremove += list(mock_results[ (mock_results[\"Method\"] == \"mindivlp_thresh\") & (mock_results[\"Parameters\"] != \"10000_14_0.01_8\") ].index.values)\n",
    "indicestoremove.sort()\n",
    "new_results = mock_results.drop(indicestoremove)\n",
    "new_results[new_results[\"Method\"] == \"mindivlp\"][\"Parameters\"].unique()\n",
    "old_results = mock_results\n",
    "mock_results = new_results\n",
    "mock_results[\"Parameters\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = pointplot_from_data_frame(mock_results, \"Level\", y_vars, \n",
    "                                  group_by=\"Reference\", color_by=\"Method\",\n",
    "                                  color_palette=color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in point.items():\n",
    "    v.savefig(join(outdir, 'mock-{0}-lineplots.png'.format(k)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
