{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation: using python to sweep over methods and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a template for using python to generate and run a list of commands. To use, follow these instructions:\n",
    "\n",
    "1) select ``File -> Make a Copy...`` from the toolbar above to copy this notebook and provide a new name describing the method(s) that you are testing.\n",
    "\n",
    "2) Modify file paths in cell 2 of [Environment preparation](#Environment-preparation) to match the directory structure on your system.\n",
    "\n",
    "3) Select the datasets you wish to test under [Preparing data set sweep](#Preparing-data-set-sweep); choose from the list of datasets included in ``tax-credit``, or add your own.\n",
    "\n",
    "4) [Prepare methods and command template](#Preparing-the-method/parameter-combinations-and-generating-commands). Enter your method / parameter combinations as a dictionary to ``method_parameters_combinations`` in cell 1, then provide a ``command_template`` in cell 2. This notebook example assumes that the method commands are passed to the command line, but the command list generated by ``parameter_sweep()`` can also be directed to the python interpreter, as shown in [this example](./taxonomy-assignment-q2-feature-classifer.ipynb). Check command list in cell 3, and set number of jobs and ``joblib`` parameters in cell 4.\n",
    "\n",
    "5) Run all cells and hold onto your hat.\n",
    "\n",
    "For an example of how to test classification methods in this notebook, see [taxonomy assignment with Qiime 1](./taxonomy-assignment-qiime1.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, expandvars \n",
    "from joblib import Parallel, delayed\n",
    "from glob import glob\n",
    "from os import system\n",
    "from tax_credit.framework_functions import (parameter_sweep,\n",
    "                                            generate_per_method_biom_tables,\n",
    "                                            move_results_to_repository)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = expandvars(\"/media/sf_Shared/tax-credit-data\")\n",
    "analysis_name= \"mock-community\"\n",
    "data_dir = join(project_dir, \"data\", analysis_name)\n",
    "\n",
    "reference_database_dir = expandvars(\"/media/sf_Shared/ref_dbs/\")\n",
    "results_dir = expandvars(\"/media/sf_Shared/tax-credit-data/computed-results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data set sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to define the data sets that we'll sweep over. The following cell does not need to be modified unless if you wish to change the datasets or reference databases used in the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reference_combinations = [\n",
    " ('mock-1', 'gg_13_8_otus'), # formerly S16S-1\n",
    "]\n",
    "\n",
    "reference_dbs = {'gg_13_8_otus' : (join(reference_database_dir, 'gg_13_8_otus/rep_set/99_otus.fasta'), \n",
    "                                   join(reference_database_dir, 'gg_13_8_otus/taxonomy/99_otu_taxonomy.txt'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the method/parameter combinations and generating commands\n",
    "\n",
    "Now we set the methods and method-specific parameters that we want to sweep. Modify to sweep other methods. Note how method_parameters_combinations feeds method/parameter combinations to parameter_sweep() in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_parameters_combinations = {\n",
    "              'mindivlp': {'small_k': [4],\n",
    "                           'large_k': [4],\n",
    "                           'q_value': [0.1],\n",
    "                           'const': [1000]\n",
    "                          }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now enter the template of the command to sweep, and generate a list of commands with ``parameter_sweep()``.\n",
    "\n",
    "Fields must adhere to following format:\n",
    "\n",
    "                      {0} = output directory\n",
    "                      {1} = input data\n",
    "                      {2} = reference sequences\n",
    "                      {3} = reference taxonomy\n",
    "                      {4} = method name\n",
    "                      {5} = other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_template = \"mkdir -p {0}; python /media/sf_Shared/MinDivLP/classify_mindivlp.py -i {1} -o {0} -r {2} -t {3} {5}\"\n",
    "        \n",
    "commands = parameter_sweep(data_dir, results_dir, reference_dbs,\n",
    "                           dataset_reference_combinations,\n",
    "                           method_parameters_combinations, command_template,\n",
    "                           infile='rep_seqs.fna', output_name='rep_seqs_tax_assignments.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can look at the first command that was generated and the number of commands generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mkdir -p /media/sf_Shared/tax-credit-data/computed-results/mock-1/gg_13_8_otus/mindivlp/1000_4_0.1_4; python /media/sf_Shared/MinDivLP/classify_mindivlp.py -i /media/sf_Shared/tax-credit-data/data/mock-community/mock-1/rep_seqs.fna -o /media/sf_Shared/tax-credit-data/computed-results/mock-1/gg_13_8_otus/mindivlp/1000_4_0.1_4 -r /media/sf_Shared/ref_dbs/gg_13_8_otus/rep_set/99_otus.fasta -t /media/sf_Shared/ref_dbs/gg_13_8_otus/taxonomy/99_otu_taxonomy.txt --const 1000 --large_k 4 --q_value 0.1 --small_k 4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(commands))\n",
    "commands[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run our commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=4)(delayed(system)(command) for command in commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move result files to repository\n",
    "\n",
    "Add results to the tax-credit directory (e.g., to push these results to the repository or compare with other precomputed results in downstream analysis steps). The precomputed_results_dir path and methods_dirs glob below should not need to be changed unless if substantial changes were made to filepaths in the preceding cells.\n",
    "\n",
    "Uncomment and run when (and if) you want to move your new results to the ``tax-credit`` directory. Note that results needn't be in ``tax-credit`` to compare using the evaluation notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_results_dir = join(project_dir, \"data\", \"precomputed-results\", analysis_name)\n",
    "method_dirs = glob(join(results_dir, '*', '*', '*', '*'))\n",
    "move_results_to_repository(method_dirs, precomputed_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
